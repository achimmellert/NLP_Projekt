{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-21T14:24:58.725682Z",
     "start_time": "2025-10-21T14:24:58.719293Z"
    }
   },
   "source": [
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skorch.callbacks import EarlyStopping, ProgressBar, LRScheduler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow\n",
    "from rich import print\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import joblib\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prüfung, ob CUDA verfügbar ist\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ],
   "id": "82b8701173730d69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T18:19:38.332545Z",
     "start_time": "2025-10-19T18:19:38.327546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ordner-Erstellung\n",
    "\n",
    "os.makedirs(\"./Plots\", exist_ok=True)\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "os.makedirs(\"./data/test_data\", exist_ok=True)\n",
    "os.makedirs(\"./data/train_data\", exist_ok=True)\n",
    "os.makedirs(\"./Models\", exist_ok=True)\n",
    "os.makedirs(\"./Models/Streamlit\", exist_ok=True)\n"
   ],
   "id": "dfb3ebeb986b84c4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T11:02:36.353750Z",
     "start_time": "2025-10-20T11:02:36.069830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mlflow Experiment initialisieren\n",
    "\n",
    "exp_id = mlflow.set_experiment(\"Classification - NLP\")\n",
    "\n",
    "run = mlflow.start_run(run_name=\"Logistic Regression und MLP\")\n",
    "run_id = run.info.run_id\n",
    "\n",
    "model_uris = {}\n"
   ],
   "id": "7f335952d24708cf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T18:22:37.881722Z",
     "start_time": "2025-10-19T18:22:04.777402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Einlesen der Train und Test CSV-Dateien für TF-IDF\n",
    "\n",
    "filename = \"amazon_review_full_csv.tar.gz\"\n",
    "filepath = Path.cwd() / \"data\" / filename\n",
    "columns = [\"Label\", \"Title\", \"Description\"]\n",
    "\n",
    "with tarfile.open(filepath, \"r:gz\") as tar:\n",
    "\n",
    "    test_file = tar.extractfile(\"amazon_review_full_csv/test.csv\")\n",
    "    df_test = pd.read_csv(test_file, header=None)\n",
    "\n",
    "    train_file = tar.extractfile(\"amazon_review_full_csv/train.csv\")\n",
    "    df_train = pd.read_csv(train_file, header=None)\n",
    "\n",
    "df_test.columns = columns\n",
    "df_train.columns = columns"
   ],
   "id": "e0742b3db48c2446",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Speicherverbrauch im RAM: {\n",
    "(df_train.memory_usage(deep=True).sum() + df_test.memory_usage(deep=True).sum())\n",
    "/ 1000000:.3f} MegaBytes\")\n"
   ],
   "id": "965ec2e09a2c9c73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Shape des gesamten Datensatzes: {pd.concat([df_train, df_test]).shape}\")\n",
   "id": "73c5be707fd07f9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T18:23:20.981510Z",
     "start_time": "2025-10-19T18:23:20.966042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Die ersten 10 Datensätze\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    sample_data = df_train.head(10)\n",
    "    local_path = os.path.join(tmp, \"sample_data.csv\")\n",
    "    sample_data.to_csv(local_path)\n",
    "    mlflow.log_artifact(local_path=local_path, artifact_path=\"Data\")\n"
   ],
   "id": "78d4c3b13c81eb81",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Klassenverteilung, um zu prüfen, ob Verteilung möglicherweise ungleich ist\n",
    "# Bei ungleicher Klassenverteilung müssen entsprechende Maßnahmen wie Under-/Oversampling ergriffen werden\n",
    "# Außerdem sollte die Metrik abhängig von der Klassenverteilung gewählt werden\n",
    "\n",
    "sns.set_theme(context=\"paper\", style=\"whitegrid\")\n",
    "\n",
    "pd.concat([df_train, df_test], ignore_index=True).iloc[:, 0].value_counts(normalize=True).sort_index().plot(kind=\"bar\", color=\"steelblue\")\n",
    "\n",
    "plt.title(\"Klassenverteilung\")\n",
    "\n",
    "plt.xlabel(\"Klassen (Review-Ratings)\")\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.ylabel(\"Anteil\")\n",
    "\n",
    "# Prozentwerte auf Balken\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., h + 0.005, f'{h:.1%}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_ylim(top=ax.get_ylim()[1] * 1.08)\n",
    "\n",
    "save_path = Path.cwd() / \"Plots\" / \"Klassenverteilung.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "mlflow.log_artifact(local_path=save_path, artifact_path=\"Plots\")\n"
   ],
   "id": "ee2be73b14663279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--> Die Klassenverteilung ist exakt ausgeglichen, daher bedarf es hierfür keine Maßnahmen.\n",
   "id": "8f6fef334ae034cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prüfung auf fehlende Einträge\n",
    "\n",
    "print(pd.concat([df_train, df_test]).isna().sum())\n",
    "\n",
    "df_test = df_test.dropna().reset_index(drop=True)\n",
    "df_train = df_train.dropna().reset_index(drop=True)\n"
   ],
   "id": "5b1e76982c26c8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "--> In 214 Zeilen fehlt der Titel der Review\n",
    "--> Da dies so gut wie nichts ist, werden diese gelöscht"
   ],
   "id": "723ed5ae15805e7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T18:24:10.453908Z",
     "start_time": "2025-10-19T18:23:47.851797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aufteilung in X und y\n",
    "# Titel und Textbeschreibung werden zu einem Feature zusammengeführt.\n",
    "\n",
    "X_train = df_train.loc[:, \"Title\"] + \" \" + df_train.loc[:, \"Description\"]\n",
    "X_test = df_test.loc[:, \"Title\"] + \" \" + df_test.loc[:, \"Description\"]\n",
    "\n",
    "y_train = df_train.loc[:, \"Label\"]\n",
    "y_test = df_test.loc[:, \"Label\"]\n",
    "\n",
    "X_train.to_csv(\"./data/train_data/X_train.csv\", header=None, index=False)\n",
    "X_test.to_csv(\"./data/test_data/X_test.csv\", header=None, index=False)\n",
    "\n",
    "y_train.to_csv(\"./data/train_data/y_train.csv\", header=None, index=False)\n",
    "y_test.to_csv(\"./data/test_data/y_test.csv\", header=None, index=False)\n"
   ],
   "id": "5729a8f2006de984",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Die ersten 5 Werte\n",
    "\n",
    "print(X_train.head(5))"
   ],
   "id": "e37473e4c3f71592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Folgender Befehl muss im Terminal ausgeführt werden, um das Spacy-Sprachpaket zu laden\n",
    "# python -m spacy download en_core_web_sm"
   ],
   "id": "2459b80176413291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training eines Logistic Regression Models mit TF-IDF als Vectorizer\n",
    "# TF-IDF von Sklearn gibt eine Sparse-Matrix aus\n",
    "\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"en_core_web_sm\", batch_size=256):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.Series):\n",
    "            X = pd.Series(X)\n",
    "\n",
    "        # Lazy-Load des Modells – nur beim Aufruf\n",
    "        nlp = spacy.load(self.model_name, disable=[\"parser\", \"ner\"])\n",
    "\n",
    "        def clean_text(text: str) -> str:\n",
    "            if not isinstance(text, str):\n",
    "                return \"\"\n",
    "            text = text.strip().lower()\n",
    "            text = re.sub(r\"\\d+\", \"\", text)\n",
    "            text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "            text = re.sub(r\"\\s+\", \" \", text)\n",
    "            return text\n",
    "\n",
    "        cleaned_text = X.apply(clean_text)\n",
    "        docs = nlp.pipe(cleaned_text, batch_size=self.batch_size)\n",
    "\n",
    "        processed_docs = [\n",
    "            \" \".join(token.lemma_ for token in doc if not token.is_stop and token.is_alpha)\n",
    "            for doc in docs\n",
    "        ]\n",
    "\n",
    "        ser = pd.Series(processed_docs, index=X.index).dropna()\n",
    "\n",
    "        return ser\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=0.001, # Wörter ignorieren, die in < 0,1 % der Dokumente vorkommen\n",
    "    max_df=0.95 # Wörter ignorieren, die in mehr als 95 % der Dokumente vorkommen (restliche Stoppwörter)\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(solver=\"saga\",\n",
    "                        penalty=\"l2\",\n",
    "                        max_iter=1000,\n",
    "                        random_state=42,\n",
    "                        verbose=1\n",
    "                        )\n",
    "\n",
    "pipeline_tfidf_lr = Pipeline([\n",
    "    (\"text_cleaner\", TextCleaner()),\n",
    "    (\"tfidf_vectorizer\", tfidf),\n",
    "    (\"logistic_regression\", lr)\n",
    "], verbose=True)\n",
    "\n",
    "\n",
    "def get_x_train():\n",
    "    return pd.read_csv(\"./data/train_data/X_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "def get_y_train():\n",
    "    return pd.read_csv(\"./data/train_data/y_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "\n",
    "X_train = get_x_train()\n",
    "y_train = get_y_train()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline_tfidf_lr.fit(X_train, y_train)\n",
    "\n",
    "overall_train_time = round(time.time() - start_time) / 60\n",
    "\n",
    "mlflow.log_metric(key=\"Training_time_lr_tfidf_minutes\", value=overall_train_time)\n",
    "\n",
    "mlflow.sklearn.log_model(pipeline_tfidf_lr, artifact_path=\"tfidf_lr_pipeline\")\n",
    "model_uris[\"tfidf_lr_pipeline\"] = f\"runs:/{run_id}/tfidf_lr_pipeline\"\n",
    "\n",
    "with open(\"class_labels.json\", \"w\") as f:\n",
    "    json.dump(pipeline_tfidf_lr.classes_.tolist(), f)\n",
    "\n",
    "mlflow.log_artifact(\"class_labels.json\", artifact_path=\"Data\")\n"
   ],
   "id": "7f08ea9595de256d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RAM leeren mit dem Python Garbage Collector\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ],
   "id": "a25f63b36501eca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mlflow.start_run(run_name=\"NochmalBertTraining\")",
   "id": "9ef39046a811adb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T09:06:48.923719Z",
     "start_time": "2025-10-21T09:06:48.920652Z"
    }
   },
   "cell_type": "code",
   "source": "run_id = mlflow.active_run().info.run_id",
   "id": "3caf458104fbe599",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Training eines Logistic Regression Models mit Sentence-BERT als Vectorizer\n",
    "# Der Sentence-BERT wird in einer Sklearn-Transformer-Klasse eingebettet, damit eine Pipeline erstellt werden kann\n",
    "\n",
    "\n",
    "# all-MiniLM-L6-v2 erstellt Vektoren mit 384 Dimensionen\n",
    "class SentenceBERTVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", batch_size=256, device=\"cuda\"):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model_ = SentenceTransformer(self.model_name, device=self.device)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not hasattr(self, \"model_\") or self.model_ is None:\n",
    "            self.model_ = SentenceTransformer(self.model_name, device=self.device)\n",
    "\n",
    "        return self.model_.encode(\n",
    "            X,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            device=self.device,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Für joblib-Speicherung: Modell nicht speichern (zu groß)\n",
    "        state = self.__dict__.copy()\n",
    "        if 'model_' in state:\n",
    "            del state['model_']\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state): # Beim Laden automatisch SentenceTransformer neu initialisieren\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "\n",
    "pipeline_bert = Pipeline([\n",
    "    ('sbert', SentenceBERTVectorizer(model_name='all-MiniLM-L6-v2', batch_size=256)),\n",
    "    ('clf', LogisticRegression(solver=\"saga\",\n",
    "                        penalty=\"l2\",\n",
    "                        max_iter=100,\n",
    "                        random_state=42,\n",
    "                               verbose=1\n",
    "                        ))\n",
    "])\n",
    "\n",
    "\n",
    "def get_x_train_trans():\n",
    "    return pd.read_csv(\"./data/train_data/X_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "def get_y_train_trans():\n",
    "    return pd.read_csv(\"./data/train_data/y_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "X_train = get_x_train_trans()\n",
    "y_train = get_y_train_trans()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline_bert.fit(X_train, y_train)\n",
    "\n",
    "overall_train_time = round(time.time() - start_time) / 60\n",
    "\n",
    "mlflow.log_metric(key=\"Training_time_lr_bert_minutes\", value=overall_train_time)\n",
    "\n",
    "mlflow.sklearn.log_model(pipeline_bert, artifact_path=\"bert_lr_pipeline\")\n",
    "model_uris[\"bert_lr_pipeline\"] = f\"runs:/{run_id}/bert_lr_pipeline\"\n"
   ],
   "id": "e3042052713be38e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RAM leeren mit dem Python Garbage Collector\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ],
   "id": "d7d7e7bac6ef3c50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Natives PyTorch MLP\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Skorch ermöglicht es, ein natives PyTorch Model in eine Sklearn-artige Pipeline einzubetten\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=5, dropout=None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbedder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nichts zu tun\n",
    "    def transform(self, X):\n",
    "        return self.model.encode(X, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "# 3. Skorch-Wrapper um PyTorch-Modell\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {device}\")\n",
    "net = NeuralNetClassifier(\n",
    "    MLPClassifier,\n",
    "    module__input_dim=384,\n",
    "    module__num_classes=5,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.001,\n",
    "    max_epochs=100,\n",
    "    batch_size=128,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"valid_loss\",\n",
    "            patience=5,\n",
    "            threshold=1e-4,\n",
    "            lower_is_better=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_skorch = Pipeline([\n",
    "    (\"embedder\", SentenceTransformerEmbedder()),\n",
    "    (\"classifier\", net)\n",
    "])\n",
    "\n",
    "\n",
    "def get_x_train_trans():\n",
    "    return pd.read_csv(\"./data/train_data/X_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "def get_y_train_trans():\n",
    "    return pd.read_csv(\"./data/train_data/y_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "\n",
    "X_train = get_x_train_trans()\n",
    "y_train = get_y_train_trans()\n",
    "\n",
    "# Skorch erwartet Labels von 0 bis C-1\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline_skorch.fit(X_train, y_train_encoded)\n",
    "\n",
    "overall_train_time = round((time.time() - start_time) / 60, None)\n",
    "print(f\"Beendet nach {overall_train_time} Minuten\")\n",
    "\n",
    "mlflow.log_metric(key=\"Training_time_MLP_bert_minutes\", value=overall_train_time)\n",
    "\n",
    "mlflow.sklearn.log_model(pipeline_skorch, artifact_path=\"bert_PyTorch_MLP_withoutDo\")\n",
    "model_uris[\"bert_PyTorch_MLP_withoutDo\"] = f\"runs:/{run_id}/bert_PyTorch_MLP_withoutDo\""
   ],
   "id": "c2f2fcbdb97a3210",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(net.callbacks)",
   "id": "7dface3aeca8057",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def get_x_test():\n",
    "    return pd.read_csv(\"./data/test_data/X_test.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "\n",
    "def get_y_test():\n",
    "    return pd.read_csv(\"./data/test_data/y_test.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "\n",
    "def get_models_from_mlflow():\n",
    "    models = {}\n",
    "    for uri in model_uris.keys():\n",
    "        name = uri.rsplit(\"/\", 1)[-1]\n",
    "        models[name] = mlflow.sklearn.load_model(uri)\n",
    "\n",
    "\n",
    "# Extract Model-Pipelines from Mlflow URIs\n",
    "model_pipelines = get_models_from_mlflow()\n",
    "\n",
    "for name, model in model_pipelines:\n",
    "\n",
    "    X_test = get_x_test()\n",
    "    y_test = get_y_test()\n",
    "\n",
    "    # Die Skorch-Pipeline wurde auf die Labels 0, 1, 2, 3, 4 trainiert\n",
    "    if \"MLP\" in name:\n",
    "        print(\"LabelEncoder anwenden...\")\n",
    "        le = LabelEncoder()\n",
    "        y_test = le.fit_transform(y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    ac_score = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average=\"macro\")\n",
    "\n",
    "    cr = pd.DataFrame(classification_report(y_true=y_test, y_pred=y_pred, output_dict=True))\n",
    "\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    fig = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    fig.plot()\n",
    "    plt.savefig(f\"./Plots/confusion_matrix_{name}.png\", dpi=300)\n",
    "    mlflow.log_artifact(f\"./Plots/confusion_matrix_{name}.png\", artifact_path=\"Plots\")\n",
    "    plt.close()\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        local_path = os.path.join(tmp, f\"classification_report{name}.csv\")\n",
    "        cr.to_csv(local_path)\n",
    "        mlflow.log_artifact(local_path, artifact_path=\"Plots\")\n",
    "\n",
    "    metrics = {\n",
    "        f\"{name}_accuracy_score\": ac_score,\n",
    "        f\"{name}_f1_score\": f1\n",
    "    }\n",
    "\n",
    "    mlflow.log_metrics(metrics=metrics)\n"
   ],
   "id": "96e5991542f3802c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sklearn Model Pipelines als .joblib-Datei speichern\n",
    "\n",
    "for name, model in model_pipelines:\n",
    "    joblib.dump(model, filename=f\"./Models/{name}.joblib\")\n",
    "\n",
    "# Mlflow Run endgültig beenden\n",
    "mlflow.end_run()\n"
   ],
   "id": "d8ccfa50faa5a8ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RAM leeren mit dem Python Garbage Collector\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ],
   "id": "3f061469defb1df2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zero-Shot-Sentiment-Analysis mit Hugging Face Transformer (Bart von Facebook)\n",
    "# Sentiment-Polaritäten\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "with tarfile.open(\"./data/amazon_review_full_csv.tar.gz\", \"r:gz\") as tf:\n",
    "    samples = pd.read_csv(tf.extractfile(\"amazon_review_full_csv/test.csv\"), header=None, nrows=50000).reset_index(drop=True)\n",
    "\n",
    "samples.columns = [\"Label\", \"Title\", \"Description\"]\n",
    "\n",
    "X_sample = samples.drop(columns=[\"Label\"])\n",
    "X_sample = X_sample.loc[:, \"Title\"] + \" \" + X_sample.loc[:, \"Description\"]\n",
    "y_sample = samples.loc[:, \"Label\"]\n",
    "\n",
    "\n",
    "mapping = {1: \"negative\", 2: \"negative\", 3: \"neutral\", 4: \"positive\", 5: \"positive\"}\n",
    "y_sample_mapped = y_sample.map(mapping).tolist()\n",
    "\n",
    "result = classifier(\n",
    "    X_sample.to_list(),\n",
    "    candidate_labels=[\"positive\", \"negative\", \"neutral\"]\n",
    ")\n",
    "\n",
    "y_pred_labels = [d[\"labels\"][0] for d in result]\n",
    "\n",
    "acc_score = accuracy_score(y_true=y_sample_mapped, y_pred=y_pred_labels)\n",
    "\n",
    "f1_score = f1_score(y_true=y_sample_mapped, y_pred=y_pred_labels, average=\"macro\")\n",
    "\n",
    "metrics = {\n",
    "    \"f1_score_Transformer\": f1_score,\n",
    "    \"accuracy_Transformer\": acc_score\n",
    "}\n",
    "\n",
    "mlflow.log_metrics(metrics)\n",
    "\n",
    "print(f\"Die Genauigkeit durch den Transformer im Zero-Shot beträgt: {acc_score * 100:.2f} %\")\n",
    "\n",
    "print(f\"Der F1-Score durch den Transformer im Zero-Shot beträgt: {f1_score * 100:.2f} %\")\n",
    "\n",
    "\n"
   ],
   "id": "cd728736f3d204ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Skorch ermöglicht es, ein natives PyTorch Model in eine Sklearn-artige Pipeline einzubetten\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Nun sind es 3 Klassen anstatt wie zuvor 5\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=3, dropout=None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbedder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nichts zu tun\n",
    "    def transform(self, X):\n",
    "        return self.model.encode(X, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "# 3. Skorch-Wrapper um PyTorch-Modell\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {device}\")\n",
    "net = NeuralNetClassifier(\n",
    "    MLPClassifier,\n",
    "    module__input_dim=384,\n",
    "    module__num_classes=3,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.001,\n",
    "    max_epochs=100,\n",
    "    batch_size=128,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"valid_loss\",\n",
    "            patience=5,\n",
    "            threshold=1e-4,\n",
    "            lower_is_better=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_skorch = Pipeline([\n",
    "    (\"embedder\", SentenceTransformerEmbedder()),\n",
    "    (\"classifier\", net)\n",
    "])\n",
    "\n",
    "\n",
    "def get_x_train_trans():\n",
    "    return pd.read_csv(\"./data/train_data/X_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "def get_y_train_trans():\n",
    "    return pd.read_csv(\"./data/train_data/y_train.csv\", header=None).squeeze(\"columns\")\n",
    "\n",
    "\n",
    "X_train = get_x_train_trans()\n",
    "y_train = get_y_train_trans()\n",
    "\n",
    "mapping = {1: \"negative\", 2: \"negative\", 3: \"neutral\", 4: \"positive\", 5: \"positive\"}\n",
    "y_sample_mapped = y_train.map(mapping).tolist()\n",
    "\n",
    "# Skorch erwartet Labels von 0 bis C-1\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_sample_mapped)\n",
    "\n",
    "\n",
    "pipeline_skorch.fit(X_train, y_train_encoded)\n",
    "\n",
    "\n",
    "X_test = pd.read_csv(\"./data/test_data/X_test.csv\", header=None).squeeze(\"columns\")\n",
    "y_test = pd.read_csv(\"./data/test_data/y_test.csv\", header=None).squeeze(\"columns\")\n",
    "y_test_mapped = y_test.map(mapping).tolist()\n",
    "\n",
    "y_test_encoded = le.transform(y_test_mapped)\n",
    "\n",
    "y_pred = pipeline_skorch.predict(X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_true=y_test_encoded, y_pred=y_pred)\n",
    "f1 = f1_score(y_true=y_test_encoded, y_pred=y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Folgender Accuracy Score konnte erzielt werden: {acc_score:.2f}\")\n",
    "print(f\"Folgender F1-Score konnte erzielt werden: {f1:.2f}\")\n"
   ],
   "id": "de7a96f557153681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Speicherung der MLP-Pipeline für die Web-Oberfläche\n",
    "import joblib\n",
    "\n",
    "# Die Model-URI ist spezifisch und muss daher individuell angepasst weren\n",
    "model_uri_MLP = f\"./mlruns/457594409970679324/models/m-7d7d43bbe4ae4d7280caa04604264fbe/artifacts\"\n",
    "model = mlflow.sklearn.load_model(model_uri_MLP)\n",
    "\n",
    "joblib.dump(model, \"./Models/Streamlit/streamlit_model.joblib\")"
   ],
   "id": "b76837533b4af0d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "92000505285090ec",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
